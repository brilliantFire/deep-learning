{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rainy Weekend with Autoencoders\n",
    "\n",
    "~** *POST UNDER CONSTRUCTION* **~\n",
    "\n",
    "I'm spending this rainy weekend learning about autoencoders using TensorFlow and Keras in Python. I'm particularly interested in one kind of autoencoder called a *variational autoencoder* (because I think it might be useful in my thesis project - more on that in another post) but before I get there, let's back up and nail down some basics...\n",
    "\n",
    "### *What is autoencoding?*\n",
    "Autoencoding is much like what it sounds in the sense that the input and 'output' are essentially the same. It's an algorithm for data compression where the functions for compression and decompression are *learned from the data*. It's considered more of a *semi-supervised* learning method as opposed to a truly *unsupervised* one since it's not entirely 'targetless'. Instead it learns the targets from the data itself.\n",
    "\n",
    "Despite all this talk of data compression, autoencoders aren't typically used for that purpose. In practice, you're much more likely to see them being used to preprocess data (as in denoising - think images but it doesn't have to be ;) ) or for dimensionality reduction. In fact, the hidden layers of simple autoencoders are doing something like principal component analysis (PCA), another method traditionally used for dimensionality reduction.\n",
    "\n",
    "### *Autoencoders*\n",
    "Generally autoencoders have three parts: an encoder, a decoder, and a 'loss' function that maps one to the other. For the simplest autoencoders - the sort that compress and then reconstruct the original inputs from the compressed representation - we can think of the 'loss' as describing the amount of information lost in the process of reconstruction. Typically when people are talking about autoencoders, they're talking about ones where the encoders and decoders are neural networks (in our case deep convnets). In training the autoencoder, we're optimizing the parameters of the neural networks to minimize the 'loss' (or distance) and we do that by stochastic gradient descent (yet another topic for another post). \n",
    "\n",
    "### *The Variational Variety*\n",
    "There's a bunch of different kinds of autoencoders but for this post I'm going to concentrate on one type called a *variational autoencoder*. Variational autoencoders (VAEs) don't learn to morph the data in and out of a compressed representation of itself like the 'vanilla' autoencoders I described above. Instead, they learn the parameters of the probability distribution that the data came from. These types of autoencoders have much in common with latent factor analysis (if you know something about that). The encoder and decoder learn models that are in terms of underlying, unobserved *latent* variables. \n",
    "\n",
    "VAEs have received a lot of attention in recent years because of their *generative* ability. Since they learn about the distribution the inputs came from, we can sample from that distribution to generate novel data. As we'll see, VAEs can also be used to cluster data in useful ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras import metrics\n",
    "from keras import backend as K   # 'generic' backend so code works with either tensorflow or theano\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check devices \n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the data. I'm working with the MNIST data in the context of the [Digit Recognizer competition on Kaggle](https://www.kaggle.com/c/digit-recognizer/data). I've downloaded the data in csv format so here I'm setting a working directory and loading the data into pandas dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directory\n",
    "import os\n",
    "os.chdir('C:\\\\Users\\\\rlvis\\\\work_MSPA\\\\MNIST') # desktop\n",
    "\n",
    "train_orig = pd.read_csv('data/train.csv')\n",
    "test_orig = pd.read_csv('data/test.csv')\n",
    "\n",
    "train_orig.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a 'label' column full of '11' to the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 'label' column in test dataset; rearrange so that columns are in the same order as in train\n",
    "test_orig['label'] = 11\n",
    "testCols = test_orig.columns.tolist()\n",
    "testCols = testCols[-1:] + testCols[:-1]\n",
    "test_orig = test_orig[testCols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we combine the original train and test sets into one dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine original train and test sets\n",
    "combined = pd.concat([train_orig, test_orig], ignore_index = True)\n",
    "\n",
    "combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autoencoder doesn't learn the classes (\"Y's\"). It uses the X's to learn about the underlying multinomial latent distribution. Even though the algorithm isn't trained in a supervised way, we can test its estimates for the parameters of the distribution on a set of data it wasn't trained on. Here we choose 10,000 random images to hold out as a validation set for testing how well the latent distribution accounts for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hold out 10000 random images as a validation/test sample\n",
    "valid = combined.sample(n = 5000, random_state = 555)\n",
    "train = combined.loc[~combined.index.isin(valid.index)]\n",
    "\n",
    "# free up some space and delete train and test\n",
    "del train_orig, test_orig, combined\n",
    "\n",
    "valid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we split the data into training and validation sets into X's and Y's and reshape the rows into 28x28 px images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(['label'], axis = 1)\n",
    "X_valid = valid.drop(['label'], axis = 1)\n",
    "\n",
    "y_train = train['label']\n",
    "y_valid = valid['label']\n",
    "\n",
    "# Normalize and reshape\n",
    "X_train = X_train.astype('float32') / 255.\n",
    "X_train = X_train.values.reshape(-1,28,28,1)\n",
    "\n",
    "X_valid = X_valid.astype('float32') / 255.\n",
    "X_valid = X_valid.values.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize some of the images..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.subplot(221)\n",
    "plt.imshow(X_train[13][:,:,0])\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.imshow(X_train[690][:,:,0])\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.imshow(X_train[2375][:,:,0])\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.imshow(X_train[42013][:,:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our variational autoencoder requires three parts:  \n",
    "\n",
    "##### 1. An encoder network that learns the parameters (mean and variance) of the underlying latent distribution  \n",
    "This encoder is a CNN with 4 conv2D layers. You'll notice it has two output layers, one for the latent distribution mean (z_mu) and the other for its variance (z_log_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (28, 28, 1)    # for MNIST\n",
    "batch_size = 16\n",
    "latent_dim = 2  # Number of latent dimension parameters\n",
    "\n",
    "# Encoder architecture: Input -> Conv2D*4 -> Flatten -> Dense\n",
    "input_img = keras.Input(shape=img_shape)\n",
    "\n",
    "x = layers.Conv2D(32, 3,\n",
    "                  padding='same', \n",
    "                  activation='relu')(input_img)\n",
    "x = layers.Conv2D(64, 3,\n",
    "                  padding='same', \n",
    "                  activation='relu',\n",
    "                  strides=(2, 2))(x)\n",
    "x = layers.Conv2D(64, 3,\n",
    "                  padding='same', \n",
    "                  activation='relu')(x)\n",
    "x = layers.Conv2D(64, 3,\n",
    "                  padding='same', \n",
    "                  activation='relu')(x)\n",
    "shape_before_flattening = K.int_shape(x)\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "\n",
    "# Two outputs, latent mean and (log)variance\n",
    "z_mu = layers.Dense(latent_dim)(x)\n",
    "z_log_sigma = layers.Dense(latent_dim)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. A means of randomly sampling a point from that distribution  \n",
    "Here we make a function that does the sampling from the latent distribution described by z_mu and z_log_sigma. Because compling the model in Keras requires that everything in our VAE be a layer object, we wrap the sampling function in a Lambda layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling function\n",
    "def sampling(args):\n",
    "    z_mu, z_log_sigma = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mu)[0], latent_dim),\n",
    "                              mean=0., stddev=1.)\n",
    "    return z_mu + K.exp(z_log_sigma) * epsilon\n",
    "\n",
    "# sample vector from the latent distribution\n",
    "z = layers.Lambda(sampling)([z_mu, z_log_sigma])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. A decoder network to map that point back into reconstructed image \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder takes the latent distribution sample as input\n",
    "decoder_input = layers.Input(K.int_shape(z)[1:])\n",
    "\n",
    "# Expand to 784 total pixels\n",
    "x = layers.Dense(np.prod(shape_before_flattening[1:]),\n",
    "                 activation='relu')(decoder_input)\n",
    "\n",
    "# reshape\n",
    "x = layers.Reshape(shape_before_flattening[1:])(x)\n",
    "\n",
    "# use Conv2DTranspose to reverse the conv layers from the encoder\n",
    "x = layers.Conv2DTranspose(32, 3,\n",
    "                           padding='same', \n",
    "                           activation='relu',\n",
    "                           strides=(2, 2))(x)\n",
    "x = layers.Conv2D(1, 3,\n",
    "                  padding='same', \n",
    "                  activation='sigmoid')(x)\n",
    "\n",
    "# decoder model statement\n",
    "decoder = Model(decoder_input, x)\n",
    "\n",
    "# apply the decoder to the sample from the latent distribution\n",
    "z_decoded = decoder(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VAE is trained using two loss functions:  \n",
    "1. 'Reconstruction loss' - This is the cross-entropy describing the errors between the samples from the latent distribution and the original inputs.  \n",
    "2. The Kullback-Liebler divergence between the latent distribution and the prior.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a custom layer to calculate the loss\n",
    "class CustomVariationalLayer(keras.layers.Layer):\n",
    "\n",
    "    def vae_loss(self, x, z_decoded):\n",
    "        x = K.flatten(x)\n",
    "        z_decoded = K.flatten(z_decoded)\n",
    "        # Reconstruction loss\n",
    "        xent_loss = keras.metrics.binary_crossentropy(x, z_decoded)\n",
    "        # KL divergence\n",
    "        kl_loss = -5e-4 * K.mean(1 + z_log_sigma - K.square(z_mu) - K.exp(z_log_sigma), axis=-1)\n",
    "        return K.mean(xent_loss + kl_loss)\n",
    "\n",
    "    # adds the custom loss to the class\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        z_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, z_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        return x\n",
    "\n",
    "# apply the custom loss to the input images and the decoded latent distribution sample\n",
    "y = CustomVariationalLayer()([input_img, z_decoded])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the KL divergence acts as a regularization term. Now we instantiate the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE model statement\n",
    "vae = Model(input_img, y)\n",
    "vae.compile(optimizer='rmsprop', loss=None)\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we train our autoencoder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.fit(x=X_train, y=None,\n",
    "        shuffle=True,\n",
    "        epochs=10,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_valid, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the trained autoencoder to predict the latent distribution parameters on the validation data. I've colored them according to their 'label' class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate the inputs into the latent space\n",
    "encoder = Model(input_img, z_mu)\n",
    "x_valid_encoded = encoder.predict(X_valid, batch_size=batch_size)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(x_valid_encoded[:, 0], x_valid_encoded[:, 1], c=y_valid, cmap='brg')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another neat thing you can do is take a peak at what samples from the latent space look like as we vary the parameters. What we end up with is a smoothly varying space where each digit transforms into the others as we dial the parameters up and down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a 2D manifold of the digits\n",
    "n = 20  # figure with 20x20 digits\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "# Linearly spaced coordinates on the unit square were transformed\n",
    "# through the inverse CDF (ppf) of the Gaussian\n",
    "# to produce values of the latent variables z,\n",
    "# since the prior of the latent space is Gaussian\n",
    "grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        z_sample = np.tile(z_sample, batch_size).reshape(batch_size, 2)\n",
    "        x_decoded = decoder.predict(z_sample, batch_size=batch_size)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(figure, cmap='brg')\n",
    "plt.show()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
